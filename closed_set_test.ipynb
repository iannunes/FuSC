{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7f58e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import scipy.spatial.distance as spd\n",
    "\n",
    "from skimage import io\n",
    "from skimage import util\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.backends import cudnn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from base import *\n",
    "from models import *\n",
    "from utils import check_mkdir, evaluate, AverageMeter, CrossEntropyLoss2d, LovaszLoss, FocalLoss2d\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64f33df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_curr_metric(msk, prd, n_known):\n",
    "    \n",
    "    tru_np = msk.ravel()\n",
    "    prd_np = prd.ravel()\n",
    "    \n",
    "    tru_valid = tru_np[tru_np < (n_known + 1)]\n",
    "    prd_valid = prd_np[tru_np < (n_known + 1)]\n",
    "    \n",
    "    print('        Computing CM...')\n",
    "    cm = metrics.confusion_matrix(tru_valid, prd_valid)\n",
    "\n",
    "    print('        Computing Accs...')\n",
    "    tru_known = 0.0\n",
    "    sum_known = 0.0\n",
    "\n",
    "    for c in range(n_known):\n",
    "        tru_known += float(cm[c, c])\n",
    "        sum_known += float(cm[c, :].sum())\n",
    "\n",
    "    acc_known = float(tru_known) / float(sum_known)\n",
    "    \n",
    "    tru_unknown = float(cm[n_known, n_known])\n",
    "    sum_unknown_real = float(cm[n_known, :].sum())\n",
    "    sum_unknown_pred = float(cm[:, n_known].sum())\n",
    "    \n",
    "    pre_unknown = 0.0\n",
    "    rec_unknown = 0.0\n",
    "    \n",
    "    if sum_unknown_pred != 0.0:\n",
    "        pre_unknown = float(tru_unknown) / float(sum_unknown_pred)\n",
    "    if sum_unknown_real != 0.0:\n",
    "        rec_unknown = float(tru_unknown) / float(sum_unknown_real)\n",
    "        \n",
    "    acc_unknown = (tru_known + tru_unknown) / (sum_known + sum_unknown_real)\n",
    "    \n",
    "    acc_mean = (acc_known + acc_unknown) / 2.0\n",
    "    \n",
    "    print('        Computing Balanced Acc...')\n",
    "    bal = metrics.balanced_accuracy_score(tru_valid, prd_valid)\n",
    "    \n",
    "    print('        Computing Kappa...')\n",
    "    kap = metrics.cohen_kappa_score(tru_valid, prd_valid)\n",
    "    \n",
    "    curr_metrics = [acc_known, acc_unknown, pre_unknown, rec_unknown, bal, kap]\n",
    "    \n",
    "    return curr_metrics\n",
    "\n",
    "def get_metrics(msk, prd, num_classes):\n",
    "    msk = np.array(msk)\n",
    "    prd = np.array(prd)\n",
    "    metrics = get_curr_metric(msk, prd, num_classes)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def _fast_hist(label_pred, label_true, num_classes):\n",
    "    mask = (label_true >= 0) & (label_true < num_classes)\n",
    "    hist = np.bincount(\n",
    "        num_classes * label_true[mask].astype(int) +\n",
    "        label_pred[mask], minlength=num_classes ** 2).reshape(num_classes, num_classes)\n",
    "    return hist\n",
    "\n",
    "\n",
    "def confusion_matrix(predictions, gts, num_classes):\n",
    "    hist = np.zeros((num_classes, num_classes))\n",
    "    for lp, lt in zip(predictions, gts):\n",
    "        hist += _fast_hist(lp.flatten(), lt.flatten(), num_classes)\n",
    "        \n",
    "    return hist\n",
    "\n",
    "\n",
    "def kappa_score(confusion):\n",
    "    n_classes = confusion.shape[0]\n",
    "    sum0 = np.sum(confusion, axis=0)\n",
    "    sum1 = np.sum(confusion, axis=1)\n",
    "    expected = np.outer(sum0, sum1) / np.sum(sum0)\n",
    "\n",
    "    w_mat = np.ones([n_classes, n_classes], dtype=np.int)\n",
    "    w_mat.flat[:: n_classes + 1] = 0\n",
    "    \n",
    "    k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
    "    return 1 - k\n",
    "\n",
    "\n",
    "def evaluate(predictions, gts, num_classes):\n",
    "    hist = np.zeros((num_classes, num_classes))\n",
    "    for lp, lt in zip(predictions, gts):\n",
    "        hist += _fast_hist(lp.flatten(), lt.flatten(), num_classes)\n",
    "\n",
    "    print('metrics')\n",
    "    metrics = get_metrics(gts, predictions, num_classes)\n",
    "    print('[acc_known, acc_unknown, pre_unknown, rec_unknown, bal, kap]')\n",
    "    print(metrics)\n",
    "        \n",
    "    #print(hist)\n",
    "    # axis 0: gt, axis 1: prediction\n",
    "       \n",
    "    acc = np.diag(hist).sum() / hist.sum()\n",
    "    acc_cls = np.diag(hist) / hist.sum(axis=1)\n",
    "    acc_cls = np.nanmean(acc_cls)\n",
    "    iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n",
    "    mean_iu = np.nanmean(iu)\n",
    "    freq = hist.sum(axis=1) / hist.sum()\n",
    "    fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n",
    "    kappa = kappa_score(hist)\n",
    "    return acc, acc_cls, mean_iu, iu, fwavacc, kappa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e39f8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "############################################\n",
    "Vaihingen/Potsdam classes:\n",
    "    0 = Street\n",
    "    1 = Building\n",
    "    2 = Grass\n",
    "    3 = Tree\n",
    "    4 = Car\n",
    "    5 = Surfaces\n",
    "    6 = Boundaries\n",
    "############################################\n",
    "\n",
    "'''\n",
    "\n",
    "# Predefining directories.\n",
    "ckpt_path = './ckpt'\n",
    "outp_path = './outputs'\n",
    "\n",
    "# Setting predefined arguments.\n",
    "args = {\n",
    "    'epoch_num': 1200,            # Number of epochs.\n",
    "    'lr': 1e-3,                   # Learning rate.\n",
    "    'weight_decay': 5e-6,         # L2 penalty.\n",
    "    'momentum': 0.9,              # Momentum.\n",
    "    'batch_size': 1,              # Batch size.\n",
    "    'num_workers': 8,             # Number of workers on data loader.\n",
    "    'print_freq': 1,              # Printing frequency for mini-batch loss.\n",
    "    'w_size': 224,                # Width size for image resizing.\n",
    "    'h_size': 224,                # Height size for image resizing.\n",
    "    'test_freq': 1200,            # Test each test_freq epochs.\n",
    "    'save_freq': 1200,            # Save model each save_freq epochs.\n",
    "    'input_channels': 4,          # Number of input channels in samples/DNN.\n",
    "    'num_classes': 5,             # Number of original output classes in dataset.\n",
    "}\n",
    "\n",
    "# Reading system parameters.\n",
    "conv_name = 'unet'\n",
    "args['hidden_classes'] = '0'\n",
    "print('hidden: ' + args['hidden_classes'])\n",
    "\n",
    "dataset_name = 'Vaihingen'\n",
    "\n",
    "if dataset_name == 'Potsdam':\n",
    "    \n",
    "    args['epoch_num'] = 600\n",
    "    args['test_freq'] = args['epoch_num']\n",
    "    args['save_freq'] = args['epoch_num']\n",
    "    args['num_workers'] = 1\n",
    "    \n",
    "hidden = []\n",
    "if '_' in args['hidden_classes']:\n",
    "    hidden = [int(h) for h in args['hidden_classes'].split('_')]\n",
    "else:\n",
    "    hidden = [int(args['hidden_classes'])]\n",
    "    \n",
    "num_known_classes = args['num_classes'] - len(hidden)\n",
    "num_unknown_classes = len(hidden)\n",
    "\n",
    "\n",
    "weights = []\n",
    "weights = [1.0 for i in range(num_known_classes)]\n",
    "if 4 not in hidden:\n",
    "    weights[-1] = 2.0\n",
    "\n",
    "global_weights = torch.FloatTensor(weights)\n",
    "\n",
    "# Setting experiment name.\n",
    "exp_name = conv_name + '_' + dataset_name + '_base_dsm_' + args['hidden_classes']\n",
    "\n",
    "pretrained_path = os.path.join(ckpt_path, exp_name, 'model_' + str(args['epoch_num']) + '.pth')\n",
    "# # Setting device [0|1|2].\n",
    "# args['device'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "929e2898",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(test_loader, net, criterion, optimizer, epoch, num_known_classes, num_unknown_classes, hidden, args, save_images, save_model):\n",
    "    \n",
    "    if save_model:\n",
    "\n",
    "        torch.save(net.state_dict(), os.path.join(ckpt_path, exp_name, 'model_' + str(epoch) + '.pth'))\n",
    "        torch.save(optimizer.state_dict(), os.path.join(ckpt_path, exp_name, 'opt_' + str(epoch) + '.pth'))\n",
    "    \n",
    "    # Setting network for evaluation mode.\n",
    "    net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        preds_all = []\n",
    "        gts_all = []\n",
    "        # Creating output directory.\n",
    "        check_mkdir(os.path.join(outp_path, exp_name, 'epoch_' + str(epoch)))\n",
    "        \n",
    "        # Iterating over batches.\n",
    "        for i, data in enumerate(test_loader):\n",
    "            \n",
    "            print('Test Batch %d/%d' % (i + 1, len(test_loader)))\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            # Obtaining images, labels and paths for batch.\n",
    "#             inps_batch, labs_batch, true_batch, img_name = data\n",
    "            \n",
    "            inps_batch, labs_batch, true_batch, img_name = None, None, None, None\n",
    "            \n",
    "            if dataset_name == 'GRSS':\n",
    "                \n",
    "                # Obtaining images and labels for batch.\n",
    "                inps_batch, labs_batch, true_batch = data\n",
    "                img_name = 'image.tif'\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # Obtaining images, labels and paths for batch.\n",
    "                inps_batch, labs_batch, true_batch, img_name = data\n",
    "            \n",
    "            inps_batch = inps_batch.squeeze()\n",
    "            labs_batch = labs_batch.squeeze()\n",
    "            true_batch = true_batch.squeeze()\n",
    "            \n",
    "            # Iterating over patches inside batch.\n",
    "            for j in range(inps_batch.size(0)):\n",
    "                \n",
    "                #print('    Test MiniBatch %d/%d' % (j + 1, inps_batch.size(0)))\n",
    "                #sys.stdout.flush()\n",
    "                \n",
    "                tic = time.time()\n",
    "                \n",
    "                for k in range(inps_batch.size(1)):\n",
    "                    \n",
    "                    inps = inps_batch[j, k].unsqueeze(0)\n",
    "                    labs = labs_batch[j, k].unsqueeze(0)\n",
    "                    true = true_batch[j, k].unsqueeze(0)\n",
    "                    \n",
    "                    # Casting tensors to cuda.\n",
    "#                     inps, labs, true = inps.cuda(args['device']), labs.cuda(args['device']), true.cuda(args['device'])\n",
    "                    \n",
    "                    # Casting to cuda variables.\n",
    "                    inps = inps.cuda()#args['device'])\n",
    "                    labs = labs.cuda()#args['device'])\n",
    "                    true = true.cuda()#args['device'])\n",
    "                    \n",
    "                    # Forwarding.\n",
    "                    if conv_name == 'unet':\n",
    "                        outs, dec1, dec2, dec3, dec4 = net(inps, feat=True)\n",
    "                    elif conv_name == 'fcnresnet50':\n",
    "                        outs, classif1, fv2 = net(inps, feat=True)\n",
    "                    elif conv_name == 'fcnresnext50':\n",
    "                        outs, classif1, fv2 = net(inps, feat=True)\n",
    "                    elif conv_name == 'fcnwideresnet50':\n",
    "                        outs, classif1, fv2 = net(inps, feat=True)\n",
    "                    elif conv_name == 'fcndensenet121':\n",
    "                        outs, classif1, fv2 = net(x=inps, feat=True)\n",
    "                    elif conv_name == 'fcndensenet121pretrained':\n",
    "                        outs, classif1, fv2 = net(inps, feat=True)\n",
    "                    elif conv_name == 'fcnvgg19':\n",
    "                        outs, classif1, fv3 = net(inps, feat=True)\n",
    "                    elif conv_name == 'fcnvgg19pretrained':\n",
    "                        outs, classif1, fv3 = net(inps, feat=True)\n",
    "                    elif conv_name == 'fcninceptionv3':\n",
    "                        outs, classif1, fv4 = net(inps, feat=True)\n",
    "                    elif conv_name == 'fcnmobilenetv2':\n",
    "                        outs, classif1, fv3 = net(inps, feat=True)\n",
    "                    elif conv_name == 'segnet':\n",
    "                        outs, x_10d, x_20d = net(inps, feat=True)\n",
    "                    \n",
    "                    # Computing probabilities.\n",
    "                    soft_outs = F.softmax(outs, dim=1)\n",
    "                    \n",
    "                    # Obtaining prior predictions.\n",
    "                    prds = soft_outs.data.max(1)[1]\n",
    "                    \n",
    "                    # Obtaining posterior predictions.\n",
    "                    if conv_name == 'unet':\n",
    "                        feat_flat = torch.cat([outs, dec1, dec2, dec3], 1)\n",
    "                    elif conv_name == 'fcnresnet50':\n",
    "                        feat_flat = torch.cat([outs, classif1, fv2], 1)\n",
    "                    elif conv_name == 'fcnresnext50':\n",
    "                        feat_flat = torch.cat([outs, classif1, fv2], 1)\n",
    "                    elif conv_name == 'fcnwideresnet50':\n",
    "                        feat_flat = torch.cat([outs, classif1, fv2], 1)\n",
    "                    elif conv_name == 'fcndensenet121':\n",
    "                        feat_flat = torch.cat([outs, classif1, fv2], 1)\n",
    "                    elif conv_name == 'fcndensenet121pretrained':\n",
    "                        feat_flat = torch.cat([outs, classif1, fv2], 1)\n",
    "                    elif conv_name == 'fcnvgg19':\n",
    "                        feat_flat = torch.cat([outs, classif1, fv3], 1)\n",
    "                    elif conv_name == 'fcnvgg19pretrained':\n",
    "                        feat_flat = torch.cat([outs, classif1, fv3], 1)\n",
    "                    elif conv_name == 'fcninceptionv3':\n",
    "                        feat_flat = torch.cat([outs, classif1, fv4], 1)\n",
    "                    elif conv_name == 'fcnmobilenetv2':\n",
    "                        feat_flat = torch.cat([outs, classif1, fv3], 1)\n",
    "                    elif conv_name == 'segnet':\n",
    "                        feat_flat = torch.cat([outs, x_10d, x_20d], 1)\n",
    "                    \n",
    "                    feat_flat = feat_flat.permute(0, 2, 3, 1).contiguous().view(feat_flat.size(0) * feat_flat.size(2) * feat_flat.size(3), feat_flat.size(1)).cpu().numpy()\n",
    "                    prds_flat = prds.cpu().numpy().ravel()\n",
    "                    true_flat = true.cpu().numpy().ravel()\n",
    "                    \n",
    "                    # Appending images for epoch loss calculation.\n",
    "                    inps_np = inps.detach().squeeze(0).cpu().numpy()\n",
    "                    labs_np = labs.detach().squeeze(0).cpu().numpy()\n",
    "                    true_np = true.detach().squeeze(0).cpu().numpy()\n",
    "\n",
    "                    preds_all.append(prds.detach().squeeze(0).cpu().numpy())\n",
    "                    gts_all.append(true_np)\n",
    "                    \n",
    "                    # Saving predictions.\n",
    "                    if (save_images):\n",
    "                        \n",
    "                        if dataset_name == 'GRSS':\n",
    "                            imag_path = os.path.join(outp_path, exp_name, 'epoch_' + str(epoch), img_name.replace('.tif', '_img_' + str(j) + '_' + str(k) + '.png'))\n",
    "                            mask_path = os.path.join(outp_path, exp_name, 'epoch_' + str(epoch), img_name.replace('.tif', '_msk_' + str(j) + '_' + str(k) + '.png'))\n",
    "                            true_path = os.path.join(outp_path, exp_name, 'epoch_' + str(epoch), img_name.replace('.tif', '_tru_' + str(j) + '_' + str(k) + '.png'))\n",
    "                            pred_path = os.path.join(outp_path, exp_name, 'epoch_' + str(epoch), img_name.replace('.tif', '_prd_' + str(j) + '_' + str(k) + '.png'))\n",
    "                        else:\n",
    "                            imag_path = os.path.join(outp_path, exp_name, 'epoch_' + str(epoch), img_name[0].replace('.tif', '_img_' + str(j) + '_' + str(k) + '.png'))\n",
    "                            mask_path = os.path.join(outp_path, exp_name, 'epoch_' + str(epoch), img_name[0].replace('.tif', '_msk_' + str(j) + '_' + str(k) + '.png'))\n",
    "                            true_path = os.path.join(outp_path, exp_name, 'epoch_' + str(epoch), img_name[0].replace('.tif', '_tru_' + str(j) + '_' + str(k) + '.png'))\n",
    "                            pred_path = os.path.join(outp_path, exp_name, 'epoch_' + str(epoch), img_name[0].replace('.tif', '_prd_' + str(j) + '_' + str(k) + '.png'))\n",
    "                        \n",
    "                        if inps_np.shape[0] == 4:\n",
    "                            inps_np = inps_np[:3,:,:]\n",
    "                        \n",
    "                        inps_np = ((np.transpose(inps_np, (1, 2, 0)) + 0.5) * 255).astype(np.uint8)\n",
    "                        \n",
    "#                         print('inps_np', inps_np.shape, inps_np.min(), inps_np.max())\n",
    "#                         print('labs_np', labs_np.shape)\n",
    "#                         print('true_np', true_np.shape)\n",
    "#                         print('prds', prds.cpu().squeeze().numpy().shape)\n",
    "                        \n",
    "                        io.imsave(imag_path, inps_np)\n",
    "#                         io.imsave(imag_path, util.img_as_ubyte((np.transpose(inps_np, (1, 2, 0)) + 0.5) * 255))\n",
    "                        io.imsave(mask_path, util.img_as_ubyte(labs_np))\n",
    "                        io.imsave(true_path, util.img_as_ubyte(true_np))\n",
    "                        io.imsave(pred_path, util.img_as_ubyte(prds.cpu().squeeze().numpy()))\n",
    "                \n",
    "                toc = time.time()\n",
    "                #print('        Elapsed Time: %.2f' % (toc - tic))\n",
    "        \n",
    "        sys.stdout.flush()\n",
    "\n",
    "        acc, acc_cls, mean_iou, iou, fwavacc, kappa = evaluate(preds_all, gts_all, num_known_classes)\n",
    "        print('[acc %.4f], [acc_cls %.4f], [iou %.4f], [fwavacc %.4f], [kappa %.4f]' % (acc, acc_cls, mean_iou, fwavacc, kappa))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a0d28d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained weights from file \"./ckpt/unet_Vaihingen_base_dsm_0/model_1200.pth\"\n",
      "self.n_classes 4\n",
      "self.hidden_classes [0]\n",
      "self.n_classes 4\n",
      "self.hidden_classes [0]\n",
      "Test Batch 1/5\n",
      "Test Batch 2/5\n",
      "Test Batch 3/5\n",
      "Test Batch 4/5\n",
      "Test Batch 5/5\n",
      "metrics\n",
      "        Computing CM...\n",
      "        Computing Accs...\n",
      "        Computing Balanced Acc...\n",
      "        Computing Kappa...\n",
      "[acc_known, acc_unknown, pre_unknown, rec_unknown, bal, kap]\n",
      "[0.8681293560935579, 0.6466960467338152, 0.0, 0.0, 0.6695770367890546, 0.5391024317265145]\n",
      "[acc 0.8681], [acc_cls 0.8370], [iou 0.7509], [fwavacc 0.7725], [kappa 0.8048]\n"
     ]
    }
   ],
   "source": [
    "# Setting network architecture.\n",
    "if (conv_name == 'unet'):\n",
    "\n",
    "    net = UNet(args['input_channels'], num_classes=args['num_classes'], hidden_classes=hidden).cuda()#args['device'])\n",
    "\n",
    "elif (conv_name == 'fcnwideresnet50'):\n",
    "\n",
    "    net = FCNWideResNet50(args['input_channels'], num_classes=args['num_classes'], pretrained=False, skip=True, hidden_classes=hidden).cuda()#args['device'])\n",
    "\n",
    "elif (conv_name == 'fcndensenet121'):\n",
    "\n",
    "    net = FCNDenseNet121(args['input_channels'], num_classes=args['num_classes'], pretrained=False, skip=True, hidden_classes=hidden).cuda()#args['device'])\n",
    "\n",
    "print('Loading pretrained weights from file \"' + pretrained_path + '\"')\n",
    "net.load_state_dict(torch.load(pretrained_path))\n",
    "\n",
    "#net = nn.DataParallel(net)\n",
    "#print(net)\n",
    "sys.stdout.flush()\n",
    "\n",
    "curr_epoch = 1\n",
    "args['best_record'] = {'epoch': 0, 'lr': 1e-4, 'val_loss': 1e10, 'acc': 0, 'acc_cls': 0, 'iou': 0}\n",
    "\n",
    "# Setting datasets.\n",
    "train_set = ListDataset(dataset_name, 'Train', (args['h_size'], args['w_size']), 'statistical', hidden, overlap=False, use_dsm=True, dataset_path='../datasets/')\n",
    "train_loader = DataLoader(train_set, batch_size=args['batch_size'], num_workers=args['num_workers'], shuffle=True)\n",
    "\n",
    "test_set = ListDataset(dataset_name, 'Test', (args['h_size'], args['w_size']), 'statistical', hidden, overlap=True, use_dsm=True, dataset_path='../datasets/')\n",
    "test_loader = DataLoader(test_set, batch_size=1, num_workers=args['num_workers'], shuffle=False)\n",
    "\n",
    "# Setting criterion.\n",
    "criterion = CrossEntropyLoss2d(weight=global_weights, size_average=False, ignore_index=args['num_classes']).cuda()#args['device'])\n",
    "\n",
    "# Setting optimizer.\n",
    "#     optimizer = optim.SGD(net.parameters(), lr=args['lr'], momentum=args['momentum'], weight_decay=args['weight_decay'])\n",
    "optimizer = optim.Adam([\n",
    "    {'params': [param for name, param in net.named_parameters() if name[-4:] == 'bias'],\n",
    "     'lr': 2 * args['lr']},\n",
    "    {'params': [param for name, param in net.named_parameters() if name[-4:] != 'bias'],\n",
    "     'lr': args['lr'], 'weight_decay': args['weight_decay']}\n",
    "], betas=(args['momentum'], 0.99))\n",
    "\n",
    "#     scheduler = None\n",
    "#     if dataset_name == 'GRSS':\n",
    "#         scheduler = optim.lr_scheduler.StepLR(optimizer, args['epoch_num'] // 5, 0.2)\n",
    "#     elif dataset_name == 'iSAID':\n",
    "#         scheduler = optim.lr_scheduler.StepLR(optimizer, args['epoch_num'] // 5, 0.2)\n",
    "#     else:\n",
    "#         scheduler = optim.lr_scheduler.StepLR(optimizer, args['epoch_num'] // 3, 0.2)\n",
    "\n",
    "# Making sure checkpoint and output directories are created.\n",
    "check_mkdir(ckpt_path)\n",
    "check_mkdir(os.path.join(ckpt_path, exp_name))\n",
    "check_mkdir(outp_path)\n",
    "check_mkdir(os.path.join(outp_path, exp_name))\n",
    "\n",
    "# Writing training args to experiment log file.\n",
    "open(os.path.join(ckpt_path, exp_name, str(datetime.datetime.now()) + '.txt'), 'w').write(str(args) + '\\n\\n')\n",
    "\n",
    "# Computing test.\n",
    "test(test_loader, net, criterion, optimizer, args['epoch_num'], num_known_classes, num_unknown_classes, hidden, args, True, False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
